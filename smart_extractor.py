#!/usr/bin/env python3
"""
Smart Knowledge Base Extractor
Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð’Ð¡Ð• Ñ‚Ð¸Ð¿Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¸ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹
"""

import re
import json
import csv
from typing import Dict, List, Any
from datetime import datetime
import os

class SmartKnowledgeExtractor:
    def __init__(self, text_file='extracted_text.txt'):
        self.text_file = text_file
        self.text = self.load_text()
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ð°Ð¿ÐºÑƒ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð±Ð°Ð· Ð·Ð½Ð°Ð½Ð¸Ð¹
        self.output_dir = 'fleethand_knowledge_bases'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð²ÑÐµ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹
        self.knowledge_bases = {
            'endpoints': [],         # Ð’ÑÐµ API endpoints
            'models': [],           # ÐœÐ¾Ð´ÐµÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ ÑÑ…ÐµÐ¼Ñ‹
            'parameters': [],       # Ð’ÑÐµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾
            'examples': [],         # ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ ÐºÐ¾Ð´Ð° Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
            'errors': [],          # ÐšÐ¾Ð´Ñ‹ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¸ Ð¸Ñ… Ñ€ÐµÑˆÐµÐ½Ð¸Ñ
            'auth': [],            # ÐœÐµÑ‚Ð¾Ð´Ñ‹ Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
            'webhooks': [],        # Webhook ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ
            'rate_limits': [],     # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð¸ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹
            'business_rules': [],  # Ð‘Ð¸Ð·Ð½ÐµÑ-Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð° Ð¸ Ð»Ð¾Ð³Ð¸ÐºÐ°
            'glossary': [],        # Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð¾Ð²
            'workflows': [],       # Ð¢Ð¸Ð¿Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ
            'integrations': [],    # Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸ Ñ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ð¼Ð¸
            'permissions': [],     # Ð Ð¾Ð»Ð¸ Ð¸ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ñ
            'data_types': [],      # Ð¢Ð¸Ð¿Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹
            'validations': []      # ÐŸÑ€Ð°Ð²Ð¸Ð»Ð° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸
        }
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ
        self.stats = {}
    
    def load_text(self):
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ñ‚ÐµÐºÑÑ‚ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸"""
        with open(self.text_file, 'r', encoding='utf-8') as f:
            return f.read()
    
    # ========== Ð˜Ð—Ð’Ð›Ð•ÐšÐÐ¢Ð•Ð›Ð˜ Ð—ÐÐÐÐ˜Ð™ ==========
    
    def extract_endpoints(self):
        """1. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²ÑÐµ API endpoints"""
        print("ðŸ“ Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Endpoints...")
        
        # ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°
        endpoint_pattern = r'(GET|POST|PUT|DELETE|PATCH)\s+(/api/v\d+/[\w/\{\}:-]+)'
        
        # Ð˜Ñ‰ÐµÐ¼ Ð²ÑÐµ endpoints
        for match in re.finditer(endpoint_pattern, self.text):
            method = match.group(1)
            path = match.group(2)
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð²Ð¾ÐºÑ€ÑƒÐ³ endpoint (500 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð´Ð¾ Ð¸ 2000 Ð¿Ð¾ÑÐ»Ðµ)
            start = max(0, match.start() - 500)
            end = min(len(self.text), match.end() + 2000)
            context = self.text[start:end]
            
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð´ÐµÑ‚Ð°Ð»Ð¸ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
            endpoint_data = {
                'id': f"{method}_{path.replace('/', '_')}",
                'method': method,
                'path': path,
                'category': self.detect_category(path),
                'description': self.extract_description(context),
                'summary': self.extract_summary(context),
                'auth_required': self.detect_auth(context),
                'deprecated': 'deprecated' in context.lower(),
                'request_type': self.extract_request_type(context),
                'response_type': self.extract_response_type(context),
                'tags': self.extract_tags(path, context),
                'related_models': self.extract_related_models(context),
                'complexity': self.assess_complexity(context)
            }
            
            self.knowledge_bases['endpoints'].append(endpoint_data)
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['endpoints'])} endpoints")
    
    def extract_models(self):
        """2. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ ÑÑ…ÐµÐ¼Ñ‹"""
        print("ðŸ“Š Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Models & Schemas...")
        
        # ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
        patterns = [
            r'(?:Model|Schema|Type|Interface|Entity):\s*(\w+)',
            r'```(?:json|typescript|javascript)\n(\{[^`]+\})\n```',
            r'"(\w+)":\s*\{[^}]+\}',
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, self.text, re.MULTILINE | re.DOTALL):
                # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ
                if '{' in match.group(0):
                    try:
                        # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ ÐºÐ°Ðº JSON
                        json_str = re.search(r'\{.*\}', match.group(0), re.DOTALL).group()
                        model_data = {
                            'model_name': self.extract_model_name(match.group(0)),
                            'structure': json_str,
                            'fields': self.parse_model_fields(json_str),
                            'required_fields': self.extract_required_fields(match.group(0)),
                            'relationships': self.extract_relationships(json_str),
                            'validation_rules': self.extract_validation_rules(match.group(0))
                        }
                        self.knowledge_bases['models'].append(model_data)
                    except:
                        pass
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['models'])} Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹")
    
    def extract_parameters(self):
        """3. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²ÑÐµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾"""
        print("ðŸ”§ Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Parameters...")
        
        # ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
        param_sections = re.findall(
            r'(?:Parameters?|Query Parameters?|Path Parameters?|Body Parameters?)[:\s]*\n((?:[-â€¢*]\s*.+\n?)+)',
            self.text, re.IGNORECASE | re.MULTILINE
        )
        
        for section in param_sections:
            params = re.findall(r'[-â€¢*]\s*`?(\w+)`?\s*[:\-â€“]\s*(.+?)(?:\n|$)', section)
            for param_name, param_desc in params:
                param_data = {
                    'name': param_name,
                    'description': param_desc.strip(),
                    'type': self.detect_param_type(param_desc),
                    'required': 'required' in param_desc.lower() or 'Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½' in param_desc.lower(),
                    'default': self.extract_default_value(param_desc),
                    'constraints': self.extract_constraints(param_desc),
                    'examples': self.extract_param_examples(param_desc),
                    'location': self.detect_param_location(section)
                }
                self.knowledge_bases['parameters'].append(param_data)
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['parameters'])} Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²")
    
    def extract_examples(self):
        """4. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²ÑÐµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ ÐºÐ¾Ð´Ð°"""
        print("ðŸ’» Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Examples...")
        
        # Ð˜Ñ‰ÐµÐ¼ Ð²ÑÐµ Ð±Ð»Ð¾ÐºÐ¸ ÐºÐ¾Ð´Ð°
        code_blocks = re.findall(
            r'(?:Example|Sample|Usage)[:\s]*\n```(\w*)\n(.*?)\n```',
            self.text, re.IGNORECASE | re.DOTALL
        )
        
        for language, code in code_blocks:
            example_data = {
                'type': 'code',
                'language': language or 'json',
                'code': code,
                'title': self.extract_example_title(code),
                'description': self.extract_example_description(code),
                'endpoint': self.detect_related_endpoint(code),
                'category': self.detect_example_category(code)
            }
            self.knowledge_bases['examples'].append(example_data)
        
        # Ð¢Ð°ÐºÐ¶Ðµ Ð¸Ñ‰ÐµÐ¼ CURL Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹
        curl_examples = re.findall(r'curl\s+.*?(?:\n\n|$)', self.text, re.DOTALL)
        for curl in curl_examples:
            self.knowledge_bases['examples'].append({
                'type': 'curl',
                'language': 'bash',
                'code': curl.strip(),
                'endpoint': self.extract_endpoint_from_curl(curl)
            })
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['examples'])} Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²")
    
    def extract_errors(self):
        """5. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ ÐºÐ¾Ð´Ñ‹ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¸ Ð¸Ñ… Ñ€ÐµÑˆÐµÐ½Ð¸Ñ"""
        print("âŒ Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Error Codes...")
        
        # ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¾ÑˆÐ¸Ð±Ð¾Ðº
        error_sections = re.findall(
            r'(?:Error Codes?|HTTP Status|Status Codes?)[:\s]*\n((?:[-â€¢*]\s*.+\n?)+)',
            self.text, re.IGNORECASE | re.MULTILINE
        )
        
        # Ð¢Ð°ÐºÐ¶Ðµ Ð¸Ñ‰ÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ñ ÐºÐ¾Ð´Ð¾Ð²
        error_codes = re.findall(r'\b([4-5]\d{2})\b\s*[:\-â€“]\s*(.+?)(?:\n|$)', self.text)
        
        for code, description in error_codes:
            error_data = {
                'code': code,
                'description': description.strip(),
                'type': self.classify_error_type(code),
                'solution': self.extract_solution(description),
                'retry_able': self.is_retryable(code),
                'user_message': self.generate_user_message(code, description)
            }
            self.knowledge_bases['errors'].append(error_data)
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['errors'])} ÐºÐ¾Ð´Ð¾Ð² Ð¾ÑˆÐ¸Ð±Ð¾Ðº")
    
    def extract_auth(self):
        """6. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸"""
        print("ðŸ” Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Authentication...")
        
        auth_sections = re.findall(
            r'(?:Authentication|Authorization|Security|API Key|Token)[:\s]*(.+?)(?:\n\n|$)',
            self.text, re.IGNORECASE | re.DOTALL
        )
        
        for section in auth_sections:
            auth_data = {
                'method': self.detect_auth_method(section),
                'description': section.strip()[:500],
                'headers': self.extract_auth_headers(section),
                'token_type': self.detect_token_type(section),
                'expiration': self.extract_token_expiration(section),
                'refresh_mechanism': self.extract_refresh_mechanism(section),
                'example': self.extract_auth_example(section)
            }
            self.knowledge_bases['auth'].append(auth_data)
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['auth'])} Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸")
    
    def extract_webhooks(self):
        """7. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ webhooks Ð¸ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ"""
        print("ðŸª Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Webhooks...")
        
        webhook_patterns = [
            r'(?:Webhook|Event|Notification)[:\s]*`?(\w+)`?',
            r'(?:event_type|eventType)["\']:\s*["\'](\w+)["\']'
        ]
        
        for pattern in webhook_patterns:
            for match in re.finditer(pattern, self.text):
                event_name = match.group(1)
                context = self.text[max(0, match.start()-300):min(len(self.text), match.end()+300)]
                
                webhook_data = {
                    'event_name': event_name,
                    'description': self.extract_description(context),
                    'payload': self.extract_webhook_payload(context),
                    'retry_policy': self.extract_retry_policy(context),
                    'headers': self.extract_webhook_headers(context)
                }
                self.knowledge_bases['webhooks'].append(webhook_data)
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['webhooks'])} webhooks")
    
    def extract_business_rules(self):
        """8. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð±Ð¸Ð·Ð½ÐµÑ-Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð° Ð¸ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ"""
        print("ðŸ“‹ Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Business Rules...")
        
        # ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»
        rule_patterns = [
            r'(?:Rule|Requirement|Constraint|Limitation|Note|Important)[:\s]*(.+?)(?:\n|$)',
            r'(?:must|should|cannot|limited to|maximum|minimum)\s+(.+?)(?:\.|$)'
        ]
        
        for pattern in rule_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                rule = match.group(1).strip()
                if len(rule) > 20 and len(rule) < 500:
                    rule_data = {
                        'rule': rule,
                        'category': self.classify_rule(rule),
                        'severity': self.assess_rule_severity(rule),
                        'affected_endpoints': self.find_affected_endpoints(rule)
                    }
                    self.knowledge_bases['business_rules'].append(rule_data)
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['business_rules'])} Ð±Ð¸Ð·Ð½ÐµÑ-Ð¿Ñ€Ð°Ð²Ð¸Ð»")
    
    def extract_workflows(self):
        """9. Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ‚Ð¸Ð¿Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ"""
        print("ðŸ”„ Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Workflows...")
        
        # Ð˜Ñ‰ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð²Ñ‹Ð·Ð¾Ð²Ð¾Ð²
        workflow_patterns = [
            r'(?:Workflow|Process|Steps?|Scenario)[:\s]*\n((?:\d+\..*\n)+)',
            r'(?:First|Then|Finally|After that)[,\s]+(.+?)(?:\.|$)'
        ]
        
        for pattern in workflow_patterns:
            for match in re.finditer(pattern, self.text, re.MULTILINE):
                workflow_data = {
                    'workflow': match.group(0),
                    'steps': self.parse_workflow_steps(match.group(0)),
                    'endpoints_sequence': self.extract_endpoints_sequence(match.group(0)),
                    'category': self.detect_workflow_category(match.group(0))
                }
                self.knowledge_bases['workflows'].append(workflow_data)
        
        print(f"   âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(self.knowledge_bases['workflows'])} workflows")
    
    # ========== Ð’Ð¡ÐŸÐžÐœÐžÐ“ÐÐ¢Ð•Ð›Ð¬ÐÐ«Ð• ÐœÐ•Ð¢ÐžÐ”Ð« ==========
    
    def detect_category(self, path):
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑŽ endpoint Ð¿Ð¾ Ð¿ÑƒÑ‚Ð¸"""
        categories = {
            'fleet': ['fleet', 'vehicle', 'car', 'truck'],
            'driver': ['driver', 'operator', 'user'],
            'task': ['task', 'job', 'assignment'],
            'report': ['report', 'analytics', 'statistics'],
            'maintenance': ['maintenance', 'service', 'repair'],
            'fuel': ['fuel', 'gas', 'consumption'],
            'route': ['route', 'trip', 'journey'],
            'location': ['location', 'position', 'gps'],
            'document': ['document', 'file', 'attachment']
        }
        
        path_lower = path.lower()
        for category, keywords in categories.items():
            if any(keyword in path_lower for keyword in keywords):
                return category
        return 'general'
    
    def extract_description(self, context):
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°"""
        desc_pattern = r'(?:Description|Summary)[:\s]*(.+?)(?:\n\n|Parameters?|Example|$)'
        match = re.search(desc_pattern, context, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()[:500]
        
        # Ð‘ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ð¾Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¿Ð¾ÑÐ»Ðµ endpoint
        sentences = re.split(r'[.!?]\s+', context)
        if sentences:
            return sentences[0].strip()[:500]
        return ""
    
    def extract_summary(self, context):
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ"""
        # Ð‘ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ð¾Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ
        first_sentence = re.search(r'^[^.!?]+[.!?]', context.strip())
        if first_sentence:
            return first_sentence.group(0).strip()
        return ""
    
    def detect_auth(self, context):
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð»Ð¸ Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ"""
        auth_indicators = ['authorization', 'authenticated', 'api key', 'token', 'bearer']
        no_auth_indicators = ['public', 'no authentication', 'anonymous']
        
        context_lower = context.lower()
        if any(indicator in context_lower for indicator in no_auth_indicators):
            return False
        if any(indicator in context_lower for indicator in auth_indicators):
            return True
        return True  # ÐŸÐ¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð°
    
    def assess_complexity(self, context):
        """ÐžÑ†ÐµÐ½Ð¸Ð²Ð°ÐµÐ¼ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ endpoint"""
        # ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ ÑÐ²Ñ€Ð¸ÑÑ‚Ð¸ÐºÐ°: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¸ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ
        param_count = len(re.findall(r'[-â€¢]\s*`?\w+`?', context))
        nested_objects = len(re.findall(r'\{[^}]*\{', context))
        
        if param_count < 3 and nested_objects == 0:
            return 'simple'
        elif param_count < 7 and nested_objects < 2:
            return 'medium'
        else:
            return 'complex'
    
    # ========== Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢ÐžÐ’ ==========
    
    def save_all_knowledge_bases(self):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð² Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹"""
        print("\nðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹...")
        
        for kb_name, kb_data in self.knowledge_bases.items():
            if not kb_data:
                continue
                
            # JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
            json_path = os.path.join(self.output_dir, f'{kb_name}.json')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(kb_data, f, indent=2, ensure_ascii=False)
            
            # CSV Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ (Ð´Ð»Ñ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð° Ð² Notion)
            csv_path = os.path.join(self.output_dir, f'{kb_name}.csv')
            if kb_data:
                keys = kb_data[0].keys()
                with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=keys)
                    writer.writeheader()
                    writer.writerows(kb_data)
            
            print(f"   âœ… {kb_name}: {len(kb_data)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ â†’ {json_path}")
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼Ð°ÑÑ‚ÐµÑ€-Ñ„Ð°Ð¹Ð» ÑÐ¾ Ð²ÑÐµÐ¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹
        master_data = {
            'api_name': 'Fleethand API',
            'version': self.extract_api_version(),
            'base_url': self.extract_base_url(),
            'generated_at': datetime.now().isoformat(),
            'statistics': self.generate_statistics(),
            'knowledge_bases': {
                name: f"{name}.json" for name in self.knowledge_bases.keys()
            }
        }
        
        with open(os.path.join(self.output_dir, 'master.json'), 'w', encoding='utf-8') as f:
            json.dump(master_data, f, indent=2)
        
        print(f"\nðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ:")
        for kb_name, kb_data in self.knowledge_bases.items():
            if kb_data:
                print(f"   â€¢ {kb_name}: {len(kb_data)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
    
    def extract_api_version(self):
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²ÐµÑ€ÑÐ¸ÑŽ API"""
        version_match = re.search(r'/v(\d+)/', self.text)
        if version_match:
            return f"v{version_match.group(1)}"
        return "v1"
    
    def extract_base_url(self):
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ URL"""
        url_match = re.search(r'https?://[^\s]+/api', self.text)
        if url_match:
            return url_match.group(0)
        return "https://api.fleethand.com"
    
    def generate_statistics(self):
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ"""
        return {
            'total_endpoints': len(self.knowledge_bases['endpoints']),
            'total_models': len(self.knowledge_bases['models']),
            'total_parameters': len(self.knowledge_bases['parameters']),
            'total_examples': len(self.knowledge_bases['examples']),
            'total_errors': len(self.knowledge_bases['errors']),
            'categories': list(set(e.get('category', '') for e in self.knowledge_bases['endpoints']))
        }
    
    # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ helper Ð¼ÐµÑ‚Ð¾Ð´Ñ‹
    def detect_param_type(self, description):
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ‚Ð¸Ð¿ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°"""
        type_keywords = {
            'string': ['string', 'text', 'str'],
            'integer': ['integer', 'int', 'number'],
            'boolean': ['boolean', 'bool', 'true/false'],
            'array': ['array', 'list', '[]'],
            'object': ['object', 'dict', '{}'],
            'datetime': ['datetime', 'date', 'timestamp'],
            'uuid': ['uuid', 'guid', 'id']
        }
        
        desc_lower = description.lower()
        for type_name, keywords in type_keywords.items():
            if any(kw in desc_lower for kw in keywords):
                return type_name
        return 'string'
    
    def classify_error_type(self, code):
        """ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¸Ð¿ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð¿Ð¾ ÐºÐ¾Ð´Ñƒ"""
        code = int(code)
        if 400 <= code < 500:
            return 'client_error'
        elif 500 <= code < 600:
            return 'server_error'
        return 'unknown'
    
    def is_retryable(self, code):
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¼Ð¾Ð¶Ð½Ð¾ Ð»Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ Ð·Ð°Ð¿Ñ€Ð¾Ñ"""
        retryable_codes = [408, 429, 502, 503, 504]
        return int(code) in retryable_codes
    
    def run(self):
        """Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ"""
        print("ðŸ§  ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸...")
        print("=" * 60)
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²ÑÐµ Ñ‚Ð¸Ð¿Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹
        self.extract_endpoints()
        self.extract_models()
        self.extract_parameters()
        self.extract_examples()
        self.extract_errors()
        self.extract_auth()
        self.extract_webhooks()
        self.extract_business_rules()
        self.extract_workflows()
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        self.save_all_knowledge_bases()
        
        print("=" * 60)
        print("âœ¨ Ð“Ð¾Ñ‚Ð¾Ð²Ð¾! Ð’ÑÐµ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹ ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ:", self.output_dir)
        print("\nðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹:")
        for file in os.listdir(self.output_dir):
            print(f"   â€¢ {file}")
        
        return self.output_dir

    # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹-Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ¸ (Ð½ÑƒÐ¶Ð½Ð¾ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸)
    def extract_request_type(self, context): return "application/json"
    def extract_response_type(self, context): return "application/json"
    def extract_tags(self, path, context): return []
    def extract_related_models(self, context): return []
    def extract_model_name(self, text): return "Model"
    def parse_model_fields(self, json_str): return []
    def extract_required_fields(self, text): return []
    def extract_relationships(self, json_str): return []
    def extract_validation_rules(self, text): return []
    def extract_default_value(self, desc): return None
    def extract_constraints(self, desc): return []
    def extract_param_examples(self, desc): return []
    def detect_param_location(self, section): return "query"
    def extract_example_title(self, code): return "Example"
    def extract_example_description(self, code): return ""
    def detect_related_endpoint(self, code): return None
    def detect_example_category(self, code): return "general"
    def extract_endpoint_from_curl(self, curl): return None
    def extract_solution(self, desc): return ""
    def generate_user_message(self, code, desc): return desc
    def detect_auth_method(self, section): return "Bearer Token"
    def extract_auth_headers(self, section): return {}
    def detect_token_type(self, section): return "JWT"
    def extract_token_expiration(self, section): return "3600"
    def extract_refresh_mechanism(self, section): return ""
    def extract_auth_example(self, section): return ""
    def extract_webhook_payload(self, context): return {}
    def extract_retry_policy(self, context): return ""
    def extract_webhook_headers(self, context): return {}
    def classify_rule(self, rule): return "general"
    def assess_rule_severity(self, rule): return "medium"
    def find_affected_endpoints(self, rule): return []
    def parse_workflow_steps(self, text): return []
    def extract_endpoints_sequence(self, text): return []
    def detect_workflow_category(self, text): return "general"

if __name__ == "__main__":
    extractor = SmartKnowledgeExtractor('extracted_text.txt')
    extractor.run()
#!/usr/bin/env python3
"""
Smart Knowledge Base Extractor
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï —Ç–∏–ø—ã –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
"""

import re
import json
import csv
from typing import Dict, List, Any
from datetime import datetime
import os

class SmartKnowledgeExtractor:
    def __init__(self, text_file='extracted_text.txt'):
        self.text_file = text_file
        self.text = self.load_text()
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤—Å–µ—Ö –±–∞–∑ –∑–Ω–∞–Ω–∏–π
        self.output_dir = 'fleethand_knowledge_bases'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        self.knowledge_bases = {
            'endpoints': [],         # –í—Å–µ API endpoints
            'models': [],           # –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ö–µ–º—ã
            'parameters': [],       # –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ç–∞–ª—å–Ω–æ
            'examples': [],         # –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤
            'errors': [],          # –ö–æ–¥—ã –æ—à–∏–±–æ–∫ –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è
            'auth': [],            # –ú–µ—Ç–æ–¥—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            'webhooks': [],        # Webhook —Å–æ–±—ã—Ç–∏—è
            'rate_limits': [],     # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ª–∏–º–∏—Ç—ã
            'business_rules': [],  # –ë–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞ –∏ –ª–æ–≥–∏–∫–∞
            'glossary': [],        # –°–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤
            'workflows': [],       # –¢–∏–ø–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            'integrations': [],    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
            'permissions': [],     # –†–æ–ª–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
            'data_types': [],      # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–æ—Ä–º–∞—Ç—ã
            'validations': []      # –ü—Ä–∞–≤–∏–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        self.stats = {}
    
    def load_text(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        with open(self.text_file, 'r', encoding='utf-8') as f:
            return f.read()
    
    # ========== –ò–ó–í–õ–ï–ö–ê–¢–ï–õ–ò –ó–ù–ê–ù–ò–ô ==========
    
    def extract_endpoints(self):
        """1. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ API endpoints"""
        print("üìç –ò–∑–≤–ª–µ–∫–∞–µ–º Endpoints...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        endpoint_pattern = r'(GET|POST|PUT|DELETE|PATCH)\s+(/api/v\d+/[\w/\{\}:-]+)'
        
        # –ò—â–µ–º –≤—Å–µ endpoints
        for match in re.finditer(endpoint_pattern, self.text):
            method = match.group(1)
            path = match.group(2)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ endpoint (500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ –∏ 2000 –ø–æ—Å–ª–µ)
            start = max(0, match.start() - 500)
            end = min(len(self.text), match.end() + 2000)
            context = self.text[start:end]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            endpoint_data = {
                'id': f"{method}_{path.replace('/', '_')}",
                'method': method,
                'path': path,
                'category': self.detect_category(path),
                'description': self.extract_description(context),
                'summary': self.extract_summary(context),
                'auth_required': self.detect_auth(context),
                'deprecated': 'deprecated' in context.lower(),
                'request_type': self.extract_request_type(context),
                'response_type': self.extract_response_type(context),
                'tags': self.extract_tags(path, context),
                'related_models': self.extract_related_models(context),
                'complexity': self.assess_complexity(context)
            }
            
            self.knowledge_bases['endpoints'].append(endpoint_data)
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['endpoints'])} endpoints")
    
    def extract_models(self):
        """2. –ò–∑–≤–ª–µ–∫–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ö–µ–º—ã"""
        print("üìä –ò–∑–≤–ª–µ–∫–∞–µ–º Models & Schemas...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π
        patterns = [
            r'(?:Model|Schema|Type|Interface|Entity):\s*(\w+)',
            r'```(?:json|typescript|javascript)\n(\{[^`]+\})\n```',
            r'"(\w+)":\s*\{[^}]+\}',
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, self.text, re.MULTILINE | re.DOTALL):
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                if '{' in match.group(0):
                    try:
                        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                        json_str = re.search(r'\{.*\}', match.group(0), re.DOTALL).group()
                        model_data = {
                            'model_name': self.extract_model_name(match.group(0)),
                            'structure': json_str,
                            'fields': self.parse_model_fields(json_str),
                            'required_fields': self.extract_required_fields(match.group(0)),
                            'relationships': self.extract_relationships(json_str),
                            'validation_rules': self.extract_validation_rules(match.group(0))
                        }
                        self.knowledge_bases['models'].append(model_data)
                    except:
                        pass
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['models'])} –º–æ–¥–µ–ª–µ–π")
    
    def extract_parameters(self):
        """3. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ç–∞–ª—å–Ω–æ"""
        print("üîß –ò–∑–≤–ª–µ–∫–∞–µ–º Parameters...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_sections = re.findall(
            r'(?:Parameters?|Query Parameters?|Path Parameters?|Body Parameters?)[:\s]*\n((?:[-‚Ä¢*]\s*.+\n?)+)',
            self.text, re.IGNORECASE | re.MULTILINE
        )
        
        for section in param_sections:
            params = re.findall(r'[-‚Ä¢*]\s*`?(\w+)`?\s*[:\-‚Äì]\s*(.+?)(?:\n|$)', section)
            for param_name, param_desc in params:
                param_data = {
                    'name': param_name,
                    'description': param_desc.strip(),
                    'type': self.detect_param_type(param_desc),
                    'required': 'required' in param_desc.lower() or '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω' in param_desc.lower(),
                    'default': self.extract_default_value(param_desc),
                    'constraints': self.extract_constraints(param_desc),
                    'examples': self.extract_param_examples(param_desc),
                    'location': self.detect_param_location(section)
                }
                self.knowledge_bases['parameters'].append(param_data)
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['parameters'])} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    def extract_examples(self):
        """4. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞"""
        print("üíª –ò–∑–≤–ª–µ–∫–∞–µ–º Examples...")
        
        # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏ –∫–æ–¥–∞
        code_blocks = re.findall(
            r'(?:Example|Sample|Usage)[:\s]*\n```(\w*)\n(.*?)\n```',
            self.text, re.IGNORECASE | re.DOTALL
        )
        
        for language, code in code_blocks:
            example_data = {
                'type': 'code',
                'language': language or 'json',
                'code': code,
                'title': self.extract_example_title(code),
                'description': self.extract_example_description(code),
                'endpoint': self.detect_related_endpoint(code),
                'category': self.detect_example_category(code)
            }
            self.knowledge_bases['examples'].append(example_data)
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º CURL –ø—Ä–∏–º–µ—Ä—ã
        curl_examples = re.findall(r'curl\s+.*?(?:\n\n|$)', self.text, re.DOTALL)
        for curl in curl_examples:
            self.knowledge_bases['examples'].append({
                'type': 'curl',
                'language': 'bash',
                'code': curl.strip(),
                'endpoint': self.extract_endpoint_from_curl(curl)
            })
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['examples'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def extract_errors(self):
        """5. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥—ã –æ—à–∏–±–æ–∫ –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è"""
        print("‚ùå –ò–∑–≤–ª–µ–∫–∞–µ–º Error Codes...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ—à–∏–±–æ–∫
        error_sections = re.findall(
            r'(?:Error Codes?|HTTP Status|Status Codes?)[:\s]*\n((?:[-‚Ä¢*]\s*.+\n?)+)',
            self.text, re.IGNORECASE | re.MULTILINE
        )
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–¥–æ–≤
        error_codes = re.findall(r'\b([4-5]\d{2})\b\s*[:\-‚Äì]\s*(.+?)(?:\n|$)', self.text)
        
        for code, description in error_codes:
            error_data = {
                'code': code,
                'description': description.strip(),
                'type': self.classify_error_type(code),
                'solution': self.extract_solution(description),
                'retry_able': self.is_retryable(code),
                'user_message': self.generate_user_message(code, description)
            }
            self.knowledge_bases['errors'].append(error_data)
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['errors'])} –∫–æ–¥–æ–≤ –æ—à–∏–±–æ–∫")
    
    def extract_auth(self):
        """6. –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–æ–¥—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        print("üîê –ò–∑–≤–ª–µ–∫–∞–µ–º Authentication...")
        
        auth_sections = re.findall(
            r'(?:Authentication|Authorization|Security|API Key|Token)[:\s]*(.+?)(?:\n\n|$)',
            self.text, re.IGNORECASE | re.DOTALL
        )
        
        for section in auth_sections:
            auth_data = {
                'method': self.detect_auth_method(section),
                'description': section.strip()[:500],
                'headers': self.extract_auth_headers(section),
                'token_type': self.detect_token_type(section),
                'expiration': self.extract_token_expiration(section),
                'refresh_mechanism': self.extract_refresh_mechanism(section),
                'example': self.extract_auth_example(section)
            }
            self.knowledge_bases['auth'].append(auth_data)
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['auth'])} –º–µ—Ç–æ–¥–æ–≤ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    
    def extract_webhooks(self):
        """7. –ò–∑–≤–ª–µ–∫–∞–µ–º webhooks –∏ —Å–æ–±—ã—Ç–∏—è"""
        print("ü™ù –ò–∑–≤–ª–µ–∫–∞–µ–º Webhooks...")
        
        webhook_patterns = [
            r'(?:Webhook|Event|Notification)[:\s]*`?(\w+)`?',
            r'(?:event_type|eventType)["\']:\s*["\'](\w+)["\']'
        ]
        
        for pattern in webhook_patterns:
            for match in re.finditer(pattern, self.text):
                event_name = match.group(1)
                context = self.text[max(0, match.start()-300):min(len(self.text), match.end()+300)]
                
                webhook_data = {
                    'event_name': event_name,
                    'description': self.extract_description(context),
                    'payload': self.extract_webhook_payload(context),
                    'retry_policy': self.extract_retry_policy(context),
                    'headers': self.extract_webhook_headers(context)
                }
                self.knowledge_bases['webhooks'].append(webhook_data)
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['webhooks'])} webhooks")
    
    def extract_business_rules(self):
        """8. –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è"""
        print("üìã –ò–∑–≤–ª–µ–∫–∞–µ–º Business Rules...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–∞–≤–∏–ª
        rule_patterns = [
            r'(?:Rule|Requirement|Constraint|Limitation|Note|Important)[:\s]*(.+?)(?:\n|$)',
            r'(?:must|should|cannot|limited to|maximum|minimum)\s+(.+?)(?:\.|$)'
        ]
        
        for pattern in rule_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                rule = match.group(1).strip()
                if len(rule) > 20 and len(rule) < 500:
                    rule_data = {
                        'rule': rule,
                        'category': self.classify_rule(rule),
                        'severity': self.assess_rule_severity(rule),
                        'affected_endpoints': self.find_affected_endpoints(rule)
                    }
                    self.knowledge_bases['business_rules'].append(rule_data)
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['business_rules'])} –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª")
    
    def extract_workflows(self):
        """9. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        print("üîÑ –ò–∑–≤–ª–µ–∫–∞–µ–º Workflows...")
        
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–∑–æ–≤–æ–≤
        workflow_patterns = [
            r'(?:Workflow|Process|Steps?|Scenario)[:\s]*\n((?:\d+\..*\n)+)',
            r'(?:First|Then|Finally|After that)[,\s]+(.+?)(?:\.|$)'
        ]
        
        for pattern in workflow_patterns:
            for match in re.finditer(pattern, self.text, re.MULTILINE):
                workflow_data = {
                    'workflow': match.group(0),
                    'steps': self.parse_workflow_steps(match.group(0)),
                    'endpoints_sequence': self.extract_endpoints_sequence(match.group(0)),
                    'category': self.detect_workflow_category(match.group(0))
                }
                self.knowledge_bases['workflows'].append(workflow_data)
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['workflows'])} workflows")
    
    # ========== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ==========
    
    def detect_category(self, path):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é endpoint –ø–æ –ø—É—Ç–∏"""
        categories = {
            'fleet': ['fleet', 'vehicle', 'car', 'truck'],
            'driver': ['driver', 'operator', 'user'],
            'task': ['task', 'job', 'assignment'],
            'report': ['report', 'analytics', 'statistics'],
            'maintenance': ['maintenance', 'service', 'repair'],
            'fuel': ['fuel', 'gas', 'consumption'],
            'route': ['route', 'trip', 'journey'],
            'location': ['location', 'position', 'gps'],
            'document': ['document', 'file', 'attachment']
        }
        
        path_lower = path.lower()
        for category, keywords in categories.items():
            if any(keyword in path_lower for keyword in keywords):
                return category
        return 'general'
    
    def extract_description(self, context):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        desc_pattern = r'(?:Description|Summary)[:\s]*(.+?)(?:\n\n|Parameters?|Example|$)'
        match = re.search(desc_pattern, context, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()[:500]
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ endpoint
        sentences = re.split(r'[.!?]\s+', context)
        if sentences:
            return sentences[0].strip()[:500]
        return ""
    
    def extract_summary(self, context):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"""
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        first_sentence = re.search(r'^[^.!?]+[.!?]', context.strip())
        if first_sentence:
            return first_sentence.group(0).strip()
        return ""
    
    def detect_auth(self, context):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"""
        auth_indicators = ['authorization', 'authenticated', 'api key', 'token', 'bearer']
        no_auth_indicators = ['public', 'no authentication', 'anonymous']
        
        context_lower = context.lower()
        if any(indicator in context_lower for indicator in no_auth_indicators):
            return False
        if any(indicator in context_lower for indicator in auth_indicators):
            return True
        return True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –Ω—É–∂–Ω–∞
    
    def assess_complexity(self, context):
        """–û—Ü–µ–Ω–∏–≤–∞–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å endpoint"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
        param_count = len(re.findall(r'[-‚Ä¢]\s*`?\w+`?', context))
        nested_objects = len(re.findall(r'\{[^}]*\{', context))
        
        if param_count < 3 and nested_objects == 0:
            return 'simple'
        elif param_count < 7 and nested_objects < 2:
            return 'medium'
        else:
            return 'complex'
    
    # ========== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ==========
    
    def save_all_knowledge_bases(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        
        for kb_name, kb_data in self.knowledge_bases.items():
            if not kb_data:
                continue
                
            # JSON —Ñ–æ—Ä–º–∞—Ç
            json_path = os.path.join(self.output_dir, f'{kb_name}.json')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(kb_data, f, indent=2, ensure_ascii=False)
            
            # CSV —Ñ–æ—Ä–º–∞—Ç (–¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –≤ Notion)
            csv_path = os.path.join(self.output_dir, f'{kb_name}.csv')
            if kb_data:
                keys = kb_data[0].keys()
                with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=keys)
                    writer.writeheader()
                    writer.writerows(kb_data)
            
            print(f"   ‚úÖ {kb_name}: {len(kb_data)} –∑–∞–ø–∏—Å–µ–π ‚Üí {json_path}")
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Ç–µ—Ä-—Ñ–∞–π–ª —Å–æ –≤—Å–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        master_data = {
            'api_name': 'Fleethand API',
            'version': self.extract_api_version(),
            'base_url': self.extract_base_url(),
            'generated_at': datetime.now().isoformat(),
            'statistics': self.generate_statistics(),
            'knowledge_bases': {
                name: f"{name}.json" for name in self.knowledge_bases.keys()
            }
        }
        
        with open(os.path.join(self.output_dir, 'master.json'), 'w', encoding='utf-8') as f:
            json.dump(master_data, f, indent=2)
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:")
        for kb_name, kb_data in self.knowledge_bases.items():
            if kb_data:
                print(f"   ‚Ä¢ {kb_name}: {len(kb_data)} –∑–∞–ø–∏—Å–µ–π")
    
    def extract_api_version(self):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Ä—Å–∏—é API"""
        version_match = re.search(r'/v(\d+)/', self.text)
        if version_match:
            return f"v{version_match.group(1)}"
        return "v1"
    
    def extract_base_url(self):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π URL"""
        url_match = re.search(r'https?://[^\s]+/api', self.text)
        if url_match:
            return url_match.group(0)
        return "https://api.fleethand.com"
    
    def generate_statistics(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return {
            'total_endpoints': len(self.knowledge_bases['endpoints']),
            'total_models': len(self.knowledge_bases['models']),
            'total_parameters': len(self.knowledge_bases['parameters']),
            'total_examples': len(self.knowledge_bases['examples']),
            'total_errors': len(self.knowledge_bases['errors']),
            'categories': list(set(e.get('category', '') for e in self.knowledge_bases['endpoints']))
        }
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ helper –º–µ—Ç–æ–¥—ã
    def detect_param_type(self, description):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–∞"""
        type_keywords = {
            'string': ['string', 'text', 'str'],
            'integer': ['integer', 'int', 'number'],
            'boolean': ['boolean', 'bool', 'true/false'],
            'array': ['array', 'list', '[]'],
            'object': ['object', 'dict', '{}'],
            'datetime': ['datetime', 'date', 'timestamp'],
            'uuid': ['uuid', 'guid', 'id']
        }
        
        desc_lower = description.lower()
        for type_name, keywords in type_keywords.items():
            if any(kw in desc_lower for kw in keywords):
                return type_name
        return 'string'
    
    def classify_error_type(self, code):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –ø–æ –∫–æ–¥—É"""
        code = int(code)
        if 400 <= code < 500:
            return 'client_error'
        elif 500 <= code < 600:
            return 'server_error'
        return 'unknown'
    
    def is_retryable(self, code):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–∂–Ω–æ –ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å"""
        retryable_codes = [408, 429, 502, 503, 504]
        return int(code) in retryable_codes
    
    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        print("üß† –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")
        print("=" * 60)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ç–∏–ø—ã –∑–Ω–∞–Ω–∏–π
        self.extract_endpoints()
        self.extract_models()
        self.extract_parameters()
        self.extract_examples()
        self.extract_errors()
        self.extract_auth()
        self.extract_webhooks()
        self.extract_business_rules()
        self.extract_workflows()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.save_all_knowledge_bases()
        
        print("=" * 60)
        print("‚ú® –ì–æ—Ç–æ–≤–æ! –í—Å–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω—ã –≤ –ø–∞–ø–∫–µ:", self.output_dir)
        print("\nüìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        for file in os.listdir(self.output_dir):
            print(f"   ‚Ä¢ {file}")
        
        return self.output_dir

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã-–∑–∞–≥–ª—É—à–∫–∏ (–Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    def extract_request_type(self, context): return "application/json"
    def extract_response_type(self, context): return "application/json"
    def extract_tags(self, path, context): return []
    def extract_related_models(self, context): return []
    def extract_model_name(self, text): return "Model"
    def parse_model_fields(self, json_str): return []
    def extract_required_fields(self, text): return []
    def extract_relationships(self, json_str): return []
    def extract_validation_rules(self, text): return []
    def extract_default_value(self, desc): return None
    def extract_constraints(self, desc): return []
    def extract_param_examples(self, desc): return []
    def detect_param_location(self, section): return "query"
    def extract_example_title(self, code): return "Example"
    def extract_example_description(self, code): return ""
    def detect_related_endpoint(self, code): return None
    def detect_example_category(self, code): return "general"
    def extract_endpoint_from_curl(self, curl): return None
    def extract_solution(self, desc): return ""
    def generate_user_message(self, code, desc): return desc
    def detect_auth_method(self, section): return "Bearer Token"
    def extract_auth_headers(self, section): return {}
    def detect_token_type(self, section): return "JWT"
    def extract_token_expiration(self, section): return "3600"
    def extract_refresh_mechanism(self, section): return ""
    def extract_auth_example(self, section): return ""
    def extract_webhook_payload(self, context): return {}
    def extract_retry_policy(self, context): return ""
    def extract_webhook_headers(self, context): return {}
    def classify_rule(self, rule): return "general"
    def assess_rule_severity(self, rule): return "medium"
    def find_affected_endpoints(self, rule): return []
    def parse_workflow_steps(self, text): return []
    def extract_endpoints_sequence(self, text): return []
    def detect_workflow_category(self, text): return "general"

if __name__ == "__main__":
    extractor = SmartKnowledgeExtractor('extracted_text.txt')
    extractor.run()
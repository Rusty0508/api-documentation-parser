#!/usr/bin/env python3
"""
Complete Knowledge Base Extractor –¥–ª—è Fleethand API
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï 15 —Ç–∏–ø–æ–≤ –∑–Ω–∞–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

import re
import json
import csv
from typing import Dict, List, Any
from datetime import datetime
import os

class CompleteFleethandExtractor:
    def __init__(self, text_file='extracted_text.txt'):
        self.text_file = text_file
        self.text = self.load_text()
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤—Å–µ—Ö –±–∞–∑ –∑–Ω–∞–Ω–∏–π
        self.output_dir = 'complete_knowledge_bases'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        self.knowledge_bases = {
            'endpoints': [],         # –í—Å–µ API endpoints
            'models': [],           # –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ö–µ–º—ã
            'parameters': [],       # –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ç–∞–ª—å–Ω–æ
            'examples': [],         # –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤
            'errors': [],          # –ö–æ–¥—ã –æ—à–∏–±–æ–∫ –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è
            'auth': [],            # –ú–µ—Ç–æ–¥—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            'webhooks': [],        # Webhook —Å–æ–±—ã—Ç–∏—è
            'rate_limits': [],     # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ª–∏–º–∏—Ç—ã
            'business_rules': [],  # –ë–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞ –∏ –ª–æ–≥–∏–∫–∞
            'glossary': [],        # –°–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤
            'workflows': [],       # –¢–∏–ø–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            'integrations': [],    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
            'permissions': [],     # –†–æ–ª–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
            'data_types': [],      # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–æ—Ä–º–∞—Ç—ã
            'validations': []      # –ü—Ä–∞–≤–∏–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        }
    
    def load_text(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        with open(self.text_file, 'r', encoding='utf-8') as f:
            return f.read()
    
    def extract_endpoints(self):
        """1. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ API endpoints"""
        print("üìç –ò–∑–≤–ª–µ–∫–∞–µ–º Endpoints...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è Fleethand
        pattern = r'Method\s*\n\s*URL\s*\n\s*(GET|POST|PUT|DELETE|PATCH)\s*\n\s*(/api/[^\s\n]+)'
        
        for match in re.finditer(pattern, self.text, re.MULTILINE):
            method = match.group(1)
            path = match.group(2)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            start = max(0, match.start() - 1000)
            end = min(len(self.text), match.end() + 2000)
            context = self.text[start:end]
            
            endpoint_data = {
                'id': f"{method}_{path.replace('/', '_').replace('-', '_')}",
                'method': method,
                'path': path,
                'category': self.detect_category(path),
                'description': self.extract_description(context),
                'summary': self.extract_summary(context),
                'auth_required': True,
                'deprecated': 'deprecated' in context.lower(),
                'request_content_type': 'application/json',
                'response_content_type': 'application/json',
                'tags': self.extract_tags(path, context),
                'complexity': self.assess_complexity(context),
                'parameters': self.extract_endpoint_parameters(context),
                'request_body': self.extract_request_body(context),
                'response_example': self.extract_response_example(context)
            }
            
            self.knowledge_bases['endpoints'].append(endpoint_data)
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['endpoints'])} endpoints")
    
    def extract_models(self):
        """2. –ò–∑–≤–ª–µ–∫–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö"""
        print("üìä –ò–∑–≤–ª–µ–∫–∞–µ–º Models & Schemas...")
        
        # –ò—â–µ–º JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        json_patterns = [
            r'\{[^{}]*"[^"]*"\s*:[^{}]*\}',  # –ü—Ä–æ—Å—Ç—ã–µ –æ–±—ä–µ–∫—Ç—ã
            r'\{[^{}]*\{[^{}]*\}[^{}]*\}',   # –í–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
            r'\[[^\[\]]*\{[^{}]*\}[^\[\]]*\]' # –ú–∞—Å—Å–∏–≤—ã –æ–±—ä–µ–∫—Ç–æ–≤
        ]
        
        model_id = 1
        for pattern in json_patterns:
            for match in re.finditer(pattern, self.text, re.DOTALL):
                json_str = match.group(0)
                
                if len(json_str) > 50 and len(json_str) < 2000:  # –†–∞–∑—É–º–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
                    try:
                        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–Ω—è—Ç—å —á—Ç–æ —ç—Ç–æ –∑–∞ –º–æ–¥–µ–ª—å
                        context = self.text[max(0, match.start()-200):min(len(self.text), match.end()+200)]
                        
                        model_data = {
                            'id': f'model_{model_id}',
                            'model_name': self.extract_model_name(context, json_str),
                            'structure': json_str,
                            'fields': self.parse_json_fields(json_str),
                            'category': self.detect_model_category(context),
                            'usage_context': context[:300],
                            'is_request': 'request' in context.lower(),
                            'is_response': 'response' in context.lower(),
                            'validation_rules': self.extract_validation_from_context(context)
                        }
                        
                        self.knowledge_bases['models'].append(model_data)
                        model_id += 1
                    except Exception as e:
                        continue
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['models'])} –º–æ–¥–µ–ª–µ–π")
    
    def extract_parameters(self):
        """3. –î–µ—Ç–∞–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
        print("üîß –ò–∑–≤–ª–µ–∫–∞–µ–º Parameters...")
        
        # –ò—â–µ–º –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        param_patterns = [
            r'(\w+Id)\s*[-‚Äì:]\s*([^.\n]+)',  # ID –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            r'(\w+)\s*\(([^)]+)\)\s*[-‚Äì:]\s*([^.\n]+)', # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å —Ç–∏–ø–∞–º–∏
            r'`(\w+)`\s*[-‚Äì:]\s*([^.\n]+)', # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ backticks
            r'(\w+)\s*parameter\s*[-‚Äì:]\s*([^.\n]+)', # –Ø–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        ]
        
        param_id = 1
        for pattern in param_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                if len(match.groups()) >= 2:
                    param_name = match.group(1)
                    param_desc = match.groups()[-1]
                    param_type = match.group(2) if len(match.groups()) > 2 else None
                    
                    if len(param_name) > 2 and len(param_desc) > 5:
                        param_data = {
                            'id': f'param_{param_id}',
                            'name': param_name,
                            'description': param_desc.strip()[:200],
                            'type': param_type or self.detect_param_type(param_desc),
                            'required': self.is_param_required(param_desc),
                            'location': self.detect_param_location(param_desc),
                            'example': self.extract_param_example(param_desc),
                            'validation': self.extract_param_validation(param_desc),
                            'default_value': self.extract_default_value(param_desc)
                        }
                        
                        self.knowledge_bases['parameters'].append(param_data)
                        param_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['parameters'])} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    def extract_examples(self):
        """4. –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞"""
        print("üíª –ò–∑–≤–ª–µ–∫–∞–µ–º Examples...")
        
        # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –ø—Ä–∏–º–µ—Ä–æ–≤
        example_patterns = [
            (r'curl\s+.*?(?=\n\n|\nMethod|\n[A-Z])', 'curl'),
            (r'```json\n(.*?)\n```', 'json'),
            (r'```\n(.*?)\n```', 'code'),
            (r'Example[:\s]*\n([^=]+?)(?=\n=|\nMethod|\n[A-Z])', 'example'),
            (r'Response[:\s]*\n([^=]+?)(?=\n=|\nMethod|\n[A-Z])', 'response')
        ]
        
        example_id = 1
        for pattern, example_type in example_patterns:
            for match in re.finditer(pattern, self.text, re.DOTALL | re.IGNORECASE):
                example_text = match.group(1) if len(match.groups()) > 0 else match.group(0)
                example_text = example_text.strip()
                
                if len(example_text) > 20 and len(example_text) < 5000:
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫ –∫–∞–∫–æ–º—É endpoint –æ—Ç–Ω–æ—Å–∏—Ç—Å—è
                    context = self.text[max(0, match.start()-500):min(len(self.text), match.end()+100)]
                    
                    example_data = {
                        'id': f'example_{example_id}',
                        'type': example_type,
                        'content': example_text,
                        'title': self.extract_example_title(context),
                        'related_endpoint': self.find_related_endpoint(context),
                        'category': self.detect_example_category(context),
                        'language': self.detect_language(example_text),
                        'description': self.extract_example_description(context)
                    }
                    
                    self.knowledge_bases['examples'].append(example_data)
                    example_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['examples'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def extract_errors(self):
        """5. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥—ã –æ—à–∏–±–æ–∫"""
        print("‚ùå –ò–∑–≤–ª–µ–∫–∞–µ–º Error Codes...")
        
        # –ò—â–µ–º HTTP –∫–æ–¥—ã –∏ –æ–ø–∏—Å–∞–Ω–∏—è –æ—à–∏–±–æ–∫
        error_patterns = [
            r'\b([4-5]\d{2})\s*[-‚Äì:]\s*([^.\n]+)',  # HTTP –∫–æ–¥—ã
            r'error[:\s]*([^.\n]+)', # –û–±—â–∏–µ –æ—à–∏–±–∫–∏
            r'Error[:\s]*([^.\n]+)', # –û—à–∏–±–∫–∏ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
            r'failed[:\s]*([^.\n]+)', # –°–±–æ–∏
        ]
        
        error_id = 1
        for pattern in error_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                if len(match.groups()) >= 2:
                    error_code = match.group(1)
                    error_desc = match.group(2)
                elif len(match.groups()) == 1:
                    error_code = f'ERR_{error_id:03d}'
                    error_desc = match.group(1)
                else:
                    continue
                
                if len(error_desc.strip()) > 10:
                    error_data = {
                        'id': f'error_{error_id}',
                        'code': error_code,
                        'description': error_desc.strip()[:200],
                        'type': self.classify_error_type(error_code),
                        'severity': self.assess_error_severity(error_desc),
                        'retryable': self.is_retryable_error(error_code),
                        'solution': self.suggest_solution(error_desc),
                        'category': self.categorize_error(error_desc)
                    }
                    
                    self.knowledge_bases['errors'].append(error_data)
                    error_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['errors'])} –æ—à–∏–±–æ–∫")
    
    def extract_auth(self):
        """6. –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        print("üîê –ò–∑–≤–ª–µ–∫–∞–µ–º Authentication...")
        
        # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∫–ª—é—á–µ–π
        auth_patterns = [
            r'(API[_\s]?[Kk]ey|Token|Bearer|Authorization)[:\s]*([^.\n]+)',
            r'(authentication|authorization)[:\s]*([^.\n]+)',
            r'(key|token|bearer)[:\s]*([^.\n]+)',
        ]
        
        auth_id = 1
        for pattern in auth_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                auth_type = match.group(1)
                auth_desc = match.group(2)
                
                if len(auth_desc.strip()) > 5:
                    auth_data = {
                        'id': f'auth_{auth_id}',
                        'method': auth_type,
                        'description': auth_desc.strip()[:300],
                        'type': self.detect_auth_type(auth_type, auth_desc),
                        'header_name': self.extract_header_name(auth_desc),
                        'format': self.extract_auth_format(auth_desc),
                        'example': self.extract_auth_example(auth_desc)
                    }
                    
                    self.knowledge_bases['auth'].append(auth_data)
                    auth_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['auth'])} –º–µ—Ç–æ–¥–æ–≤ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    
    def extract_webhooks(self):
        """7. –ò–∑–≤–ª–µ–∫–∞–µ–º webhooks –∏ —Å–æ–±—ã—Ç–∏—è"""
        print("ü™ù –ò–∑–≤–ª–µ–∫–∞–µ–º Webhooks...")
        
        webhook_patterns = [
            r'(webhook|event|notification|callback)[:\s]*([^.\n]+)',
            r'(event[_\s]?type|eventType)[:\s]*([^.\n]+)',
        ]
        
        webhook_id = 1
        for pattern in webhook_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                webhook_type = match.group(1)
                webhook_desc = match.group(2)
                
                if len(webhook_desc.strip()) > 10:
                    webhook_data = {
                        'id': f'webhook_{webhook_id}',
                        'event_name': webhook_type,
                        'description': webhook_desc.strip()[:300],
                        'trigger': self.extract_webhook_trigger(webhook_desc),
                        'payload': self.extract_webhook_payload(webhook_desc),
                        'frequency': self.extract_webhook_frequency(webhook_desc)
                    }
                    
                    self.knowledge_bases['webhooks'].append(webhook_data)
                    webhook_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['webhooks'])} webhooks")
    
    def extract_rate_limits(self):
        """8. –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ª–∏–º–∏—Ç—ã"""
        print("‚è±Ô∏è –ò–∑–≤–ª–µ–∫–∞–µ–º Rate Limits...")
        
        limit_patterns = [
            r'(limit|maximum|max|minimum|min|rate)[:\s]*([^.\n]+)',
            r'(\d+)\s*(per|/)\s*(second|minute|hour|day|request)',
            r'(timeout|delay)[:\s]*([^.\n]+)',
        ]
        
        limit_id = 1
        for pattern in limit_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                if 'limit' in match.group(0).lower() or 'rate' in match.group(0).lower():
                    limit_data = {
                        'id': f'limit_{limit_id}',
                        'type': match.group(1),
                        'description': match.group(0)[:200],
                        'value': self.extract_limit_value(match.group(0)),
                        'unit': self.extract_limit_unit(match.group(0)),
                        'scope': self.extract_limit_scope(match.group(0))
                    }
                    
                    self.knowledge_bases['rate_limits'].append(limit_data)
                    limit_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['rate_limits'])} –ª–∏–º–∏—Ç–æ–≤")
    
    def extract_business_rules(self):
        """9. –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞"""
        print("üìã –ò–∑–≤–ª–µ–∫–∞–µ–º Business Rules...")
        
        rule_patterns = [
            r'(must|should|cannot|required|mandatory|optional)[:\s]*([^.\n]+)',
            r'(rule|constraint|limitation|requirement)[:\s]*([^.\n]+)',
            r'(note|important|warning)[:\s]*([^.\n]+)',
        ]
        
        rule_id = 1
        for pattern in rule_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                rule_type = match.group(1)
                rule_desc = match.group(2)
                
                if len(rule_desc.strip()) > 20:
                    rule_data = {
                        'id': f'rule_{rule_id}',
                        'type': rule_type,
                        'description': rule_desc.strip()[:300],
                        'severity': self.assess_rule_severity(rule_type),
                        'category': self.categorize_rule(rule_desc),
                        'enforcement': self.detect_rule_enforcement(rule_type)
                    }
                    
                    self.knowledge_bases['business_rules'].append(rule_data)
                    rule_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['business_rules'])} –ø—Ä–∞–≤–∏–ª")
    
    def extract_glossary(self):
        """10. –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        print("üìñ –ò–∑–≤–ª–µ–∫–∞–µ–º Glossary...")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ API paths –∏ –æ–ø–∏—Å–∞–Ω–∏–π
        terms = set()
        
        # –ò–∑ –ø—É—Ç–µ–π API
        for endpoint in self.knowledge_bases['endpoints']:
            path_parts = endpoint['path'].split('/')
            for part in path_parts:
                if len(part) > 3 and part.isalpha():
                    terms.add(part)
        
        # –ß–∞—Å—Ç—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        common_terms = [
            'vehicle', 'driver', 'fleet', 'task', 'activity', 'document',
            'report', 'location', 'partner', 'user', 'assignment', 'fuel',
            'maintenance', 'route', 'trip', 'status', 'position'
        ]
        
        for term in common_terms:
            if term in self.text.lower():
                terms.add(term)
        
        term_id = 1
        for term in sorted(terms):
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≥–¥–µ —Ç–µ—Ä–º–∏–Ω —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è
            pattern = rf'\b{re.escape(term)}\b[^.\n]*'
            matches = re.findall(pattern, self.text, re.IGNORECASE)
            
            if matches:
                definition = matches[0][:200]
                
                glossary_data = {
                    'id': f'term_{term_id}',
                    'term': term,
                    'definition': definition,
                    'category': self.categorize_term(term),
                    'usage_count': len(matches),
                    'related_endpoints': self.find_related_endpoints_for_term(term)
                }
                
                self.knowledge_bases['glossary'].append(glossary_data)
                term_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['glossary'])} —Ç–µ—Ä–º–∏–Ω–æ–≤")
    
    def extract_workflows(self):
        """11. –ò–∑–≤–ª–µ–∫–∞–µ–º workflows –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏"""
        print("üîÑ –ò–∑–≤–ª–µ–∫–∞–µ–º Workflows...")
        
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π
        workflow_patterns = [
            r'(first|then|next|after|finally)[:\s]*([^.\n]+)',
            r'(step\s*\d+)[:\s]*([^.\n]+)',
            r'(workflow|process|scenario)[:\s]*([^.\n]+)',
        ]
        
        workflow_id = 1
        for pattern in workflow_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                step_indicator = match.group(1)
                step_desc = match.group(2)
                
                if len(step_desc.strip()) > 15:
                    workflow_data = {
                        'id': f'workflow_{workflow_id}',
                        'step_indicator': step_indicator,
                        'description': step_desc.strip()[:300],
                        'order': self.extract_step_order(step_indicator),
                        'category': self.categorize_workflow(step_desc),
                        'related_endpoints': self.find_endpoints_in_text(step_desc)
                    }
                    
                    self.knowledge_bases['workflows'].append(workflow_data)
                    workflow_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['workflows'])} workflow —à–∞–≥–æ–≤")
    
    def extract_integrations(self):
        """12. –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è—Ö"""
        print("üîó –ò–∑–≤–ª–µ–∫–∞–µ–º Integrations...")
        
        integration_patterns = [
            r'(integration|connect|sync|import|export)[:\s]*([^.\n]+)',
            r'(third[_\s]?party|external)[:\s]*([^.\n]+)',
        ]
        
        integration_id = 1
        for pattern in integration_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                integration_type = match.group(1)
                integration_desc = match.group(2)
                
                if len(integration_desc.strip()) > 20:
                    integration_data = {
                        'id': f'integration_{integration_id}',
                        'type': integration_type,
                        'description': integration_desc.strip()[:300],
                        'direction': self.detect_integration_direction(integration_desc),
                        'format': self.detect_integration_format(integration_desc)
                    }
                    
                    self.knowledge_bases['integrations'].append(integration_data)
                    integration_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['integrations'])} –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π")
    
    def extract_permissions(self):
        """13. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–æ–ª–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è"""
        print("üë• –ò–∑–≤–ª–µ–∫–∞–µ–º Permissions...")
        
        permission_patterns = [
            r'(role|permission|access|privilege)[:\s]*([^.\n]+)',
            r'(admin|user|manager|operator)[:\s]*([^.\n]+)',
        ]
        
        permission_id = 1
        for pattern in permission_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                permission_type = match.group(1)
                permission_desc = match.group(2)
                
                if len(permission_desc.strip()) > 10:
                    permission_data = {
                        'id': f'permission_{permission_id}',
                        'type': permission_type,
                        'description': permission_desc.strip()[:300],
                        'level': self.detect_permission_level(permission_type),
                        'scope': self.detect_permission_scope(permission_desc)
                    }
                    
                    self.knowledge_bases['permissions'].append(permission_data)
                    permission_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['permissions'])} —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π")
    
    def extract_data_types(self):
        """14. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö"""
        print("üèóÔ∏è –ò–∑–≤–ª–µ–∫–∞–µ–º Data Types...")
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–∏–ø—ã –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π
        data_types = set()
        
        for param in self.knowledge_bases['parameters']:
            if param.get('type'):
                data_types.add(param['type'])
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–∏–ø—ã API
        standard_types = [
            'string', 'integer', 'boolean', 'array', 'object',
            'datetime', 'uuid', 'email', 'url', 'phone'
        ]
        
        type_id = 1
        for data_type in sorted(data_types.union(standard_types)):
            type_data = {
                'id': f'type_{type_id}',
                'name': data_type,
                'description': self.describe_data_type(data_type),
                'format': self.get_type_format(data_type),
                'validation': self.get_type_validation(data_type),
                'examples': self.get_type_examples(data_type)
            }
            
            self.knowledge_bases['data_types'].append(type_data)
            type_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['data_types'])} —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
    
    def extract_validations(self):
        """15. –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        print("‚úÖ –ò–∑–≤–ª–µ–∫–∞–µ–º Validations...")
        
        validation_patterns = [
            r'(valid|invalid|validate|validation)[:\s]*([^.\n]+)',
            r'(format|pattern|regex)[:\s]*([^.\n]+)',
            r'(length|size|range)[:\s]*([^.\n]+)',
        ]
        
        validation_id = 1
        for pattern in validation_patterns:
            for match in re.finditer(pattern, self.text, re.IGNORECASE):
                validation_type = match.group(1)
                validation_desc = match.group(2)
                
                if len(validation_desc.strip()) > 10:
                    validation_data = {
                        'id': f'validation_{validation_id}',
                        'type': validation_type,
                        'description': validation_desc.strip()[:300],
                        'rule': self.extract_validation_rule(validation_desc),
                        'error_message': self.extract_validation_error(validation_desc)
                    }
                    
                    self.knowledge_bases['validations'].append(validation_data)
                    validation_id += 1
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.knowledge_bases['validations'])} –ø—Ä–∞–≤–∏–ª –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    
    # ========== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ==========
    
    def detect_category(self, path):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é endpoint"""
        categories = {
            'activities': ['activities', 'activity'],
            'drivers': ['drivers', 'driver'],
            'vehicles': ['vehicles', 'vehicle', 'fleet'],
            'reports': ['reports', 'report', 'eco'],
            'tasks': ['tasks', 'task'],
            'fuel': ['fuel', 'refuel'],
            'maintenance': ['maintenance', 'service'],
            'locations': ['locations', 'position', 'gps'],
            'documents': ['documents', 'files', 'attachments'],
            'partners': ['partners', 'partner', 'client'],
            'users': ['users', 'user'],
        }
        
        path_lower = path.lower()
        for category, keywords in categories.items():
            if any(keyword in path_lower for keyword in keywords):
                return category
        return 'general'
    
    def save_all_knowledge_bases(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ 15 –±–∞–∑ –∑–Ω–∞–Ω–∏–π...")
        
        total_records = 0
        for kb_name, kb_data in self.knowledge_bases.items():
            if kb_data:
                # JSON —Ñ–æ—Ä–º–∞—Ç
                json_path = os.path.join(self.output_dir, f'{kb_name}.json')
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(kb_data, f, indent=2, ensure_ascii=False)
                
                # CSV —Ñ–æ—Ä–º–∞—Ç
                csv_path = os.path.join(self.output_dir, f'{kb_name}.csv')
                if kb_data:
                    keys = kb_data[0].keys()
                    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                        writer = csv.DictWriter(f, fieldnames=keys)
                        writer.writeheader()
                        writer.writerows(kb_data)
                
                print(f"   ‚úÖ {kb_name}: {len(kb_data)} –∑–∞–ø–∏—Å–µ–π")
                total_records += len(kb_data)
            else:
                print(f"   ‚ö†Ô∏è {kb_name}: –ø—É—Å—Ç–∞—è –±–∞–∑–∞")
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Ç–µ—Ä-—Ñ–∞–π–ª
        master_data = {
            'api_name': 'Fleethand API',
            'version': 'v1',
            'generated_at': datetime.now().isoformat(),
            'total_records': total_records,
            'knowledge_bases': {
                name: {
                    'file': f"{name}.json",
                    'count': len(data),
                    'description': self.get_kb_description(name)
                } for name, data in self.knowledge_bases.items()
            }
        }
        
        with open(os.path.join(self.output_dir, 'master.json'), 'w', encoding='utf-8') as f:
            json.dump(master_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nüìä –ò—Ç–æ–≥–æ: {total_records} –∑–∞–ø–∏—Å–µ–π –≤ {len([kb for kb in self.knowledge_bases.values() if kb])} –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π")
        return total_records
    
    def get_kb_description(self, kb_name):
        """–û–ø–∏—Å–∞–Ω–∏—è –±–∞–∑ –∑–Ω–∞–Ω–∏–π"""
        descriptions = {
            'endpoints': '–í—Å–µ API –º–µ—Ç–æ–¥—ã —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π',
            'models': '–°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ö–µ–º—ã',
            'parameters': '–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤',
            'examples': '–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è',
            'errors': '–ö–æ–¥—ã –æ—à–∏–±–æ–∫ –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è',
            'auth': '–ú–µ—Ç–æ–¥—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏',
            'webhooks': '–°–æ–±—ã—Ç–∏—è –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è',
            'rate_limits': '–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è API',
            'business_rules': '–ë–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ –∏ –ø—Ä–∞–≤–∏–ª–∞',
            'glossary': '–°–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤',
            'workflows': '–°—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è',
            'integrations': '–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏',
            'permissions': '–†–æ–ª–∏ –∏ –ø—Ä–∞–≤–∞',
            'data_types': '–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö',
            'validations': '–ü—Ä–∞–≤–∏–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏'
        }
        return descriptions.get(kb_name, '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è')
    
    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö 15 –±–∞–∑ –∑–Ω–∞–Ω–∏–π"""
        print("üß† –ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏–∑ Fleethand API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")
        print("=" * 70)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ç–∏–ø—ã –∑–Ω–∞–Ω–∏–π
        self.extract_endpoints()        # 1
        self.extract_models()          # 2
        self.extract_parameters()      # 3
        self.extract_examples()        # 4
        self.extract_errors()          # 5
        self.extract_auth()            # 6
        self.extract_webhooks()        # 7
        self.extract_rate_limits()     # 8
        self.extract_business_rules()  # 9
        self.extract_glossary()        # 10
        self.extract_workflows()       # 11
        self.extract_integrations()    # 12
        self.extract_permissions()     # 13
        self.extract_data_types()      # 14
        self.extract_validations()     # 15
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        total_records = self.save_all_knowledge_bases()
        
        print("=" * 70)
        print("‚ú® –ì–æ—Ç–æ–≤–æ! –í—Å–µ 15 —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–∞–∑ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω—ã!")
        print(f"üìÅ –ü–∞–ø–∫–∞: {self.output_dir}/")
        print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {total_records}")
        
        return self.output_dir

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ helper –º–µ—Ç–æ–¥–æ–≤
    def extract_description(self, context): 
        return "API endpoint"
    def extract_summary(self, context): 
        return "API method"
    def extract_tags(self, path, context): 
        return [self.detect_category(path)]
    def assess_complexity(self, context): 
        return "medium"
    def extract_endpoint_parameters(self, context): 
        return []
    def extract_request_body(self, context): 
        return None
    def extract_response_example(self, context): 
        return None
    def extract_model_name(self, context, json_str): 
        return "DataModel"
    def parse_json_fields(self, json_str): 
        return []
    def detect_model_category(self, context): 
        return "general"
    def extract_validation_from_context(self, context): 
        return []
    def detect_param_type(self, desc): 
        return "string"
    def is_param_required(self, desc): 
        return False
    def detect_param_location(self, desc): 
        return "query"
    def extract_param_example(self, desc): 
        return None
    def extract_param_validation(self, desc): 
        return []
    def extract_default_value(self, desc): 
        return None
    def extract_example_title(self, context): 
        return "Example"
    def find_related_endpoint(self, context): 
        return None
    def detect_example_category(self, context): 
        return "general"
    def detect_language(self, text): 
        return "json"
    def extract_example_description(self, context): 
        return ""
    def classify_error_type(self, code): 
        return "unknown"
    def assess_error_severity(self, desc): 
        return "medium"
    def is_retryable_error(self, code): 
        return False
    def suggest_solution(self, desc): 
        return ""
    def categorize_error(self, desc): 
        return "general"
    def detect_auth_type(self, auth_type, desc): 
        return "API_KEY"
    def extract_header_name(self, desc): 
        return "Authorization"
    def extract_auth_format(self, desc): 
        return "Bearer {token}"
    def extract_auth_example(self, desc): 
        return ""
    def extract_webhook_trigger(self, desc): 
        return "unknown"
    def extract_webhook_payload(self, desc): 
        return {}
    def extract_webhook_frequency(self, desc): 
        return "on_event"
    def extract_limit_value(self, text): 
        return "unknown"
    def extract_limit_unit(self, text): 
        return "requests"
    def extract_limit_scope(self, text): 
        return "global"
    def assess_rule_severity(self, rule_type): 
        return "medium"
    def categorize_rule(self, desc): 
        return "general"
    def detect_rule_enforcement(self, rule_type): 
        return "soft"
    def categorize_term(self, term): 
        return "general"
    def find_related_endpoints_for_term(self, term): 
        return []
    def extract_step_order(self, indicator): 
        return 1
    def categorize_workflow(self, desc): 
        return "general"
    def find_endpoints_in_text(self, text): 
        return []
    def detect_integration_direction(self, desc): 
        return "bidirectional"
    def detect_integration_format(self, desc): 
        return "JSON"
    def detect_permission_level(self, perm_type): 
        return "user"
    def detect_permission_scope(self, desc): 
        return "resource"
    def describe_data_type(self, data_type): 
        return f"Data type: {data_type}"
    def get_type_format(self, data_type): 
        return "standard"
    def get_type_validation(self, data_type): 
        return []
    def get_type_examples(self, data_type): 
        return []
    def extract_validation_rule(self, desc): 
        return "unknown"
    def extract_validation_error(self, desc): 
        return "Invalid input"

if __name__ == "__main__":
    extractor = CompleteFleethandExtractor('extracted_text.txt')
    extractor.run()